задание:

Трехголовая сеть в PyTorch Задача Написать CNN для тексат. Задание на машинное обучение полностью на питоне. выполните с использованием данной репозитории https://github.com/girafe-ai/ml-course/tree/23s_advanced/homeworks_advanced/Lab02_NMT и https://github.com/girafe-ai/ml-course/blob/23s_advanced/homeworks_advanced/Lab02_NMT/lab02_Neural_Machine_Translation.ipynb напиши код, выполняющую задачу учитывая что данные репозиторий уже склонировал в рабочий проект

решение:

from lab02_Neural_Machine_Translation import *
import torch
from torch.utils.data import DataLoader
from transformers import AdamW, get_linear_schedule_with_warmup
import random

# Зададим гиперпараметры модели
EPOCHS = 10
BATCH_SIZE = 32
LEARNING_RATE = 2e-5

# Инициализируем нашу трехголовую сеть
model = ThreeHeadedModel()

# Загрузим данные для тренировки
train_dataset = TranslationDataset(train_data, src_tokenizer, tgt_tokenizer)
train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE)

# Определим функцию потерь
cross_entropy_loss = nn.CrossEntropyLoss()

# Определим оптимизатор и scheduler
optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)
total_steps = len(train_dataloader) * EPOCHS
scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)

# Зададим device нашей модели
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model.to(device)

# Начнем тренировку модели
for epoch in range(EPOCHS):
    for i, batch in enumerate(train_dataloader):
        # Получим данные и отправим их на device
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)

        # Обнулим градиенты и сделаем forward pass
        optimizer.zero_grad()
        next_word_output, masked_word_output, language_output = model(input_ids=input_ids, attention_mask=attention_mask)

        # Вычислим функцию потерь и сделаем backward pass
        next_word_loss = cross_entropy_loss(next_word_output.view(-1, 30522), batch['next_word'].view(-1).to(device))
        masked_word_loss = cross_entropy_loss(masked_word_output.view(-1, 2), batch['masked_word'].view(-1).to(device))
        language_loss = cross_entropy_loss(language_output, batch['language'].to(device))
        loss = next_word_loss + masked_word_loss + language_loss
        loss.backward()

        # Обновим веса с помощью оптимизатора и scheduler
        optimizer.step()
        scheduler.step()

        # Выведем текущую информацию о процессе тренировки
        if i % 100 == 0:
            print(f'Epoch {epoch + 1}/{EPOCHS}, Batch {i}/{len(train_dataloader)}, Loss: {loss.item()}')

