Написать сверточную нейронную сеть для обработки текстов на PyTorch, используя репозиторий https://github.com/girafe-ai/ml-course/tree/23s_advanced/homeworks_advanced/Lab02_NMT и инструкции в файле lab02_Neural_Machine_Translation.ipynb. репозитории уже скопированы в рабочую область.Нейронная сеть должна производить перевод текста на другой язык EN-RU. выполни задание по описанию и напиши готовый код 

import torch
import torch.nn as nn
import torch.optim as optim
from torchtext.datasets import Multi30k
from torchtext.data import Field, BucketIterator
import spacy
import numpy as np
import random
import math
import time

# Гиперпараметры
NUM_EPOCHS = 10
BATCH_SIZE = 128
LEARNING_RATE = 0.001
EMBEDDING_DIM = 256
HIDDEN_DIM = 512
ENCODER_NUM_LAYERS = 2
DECODER_NUM_LAYERS = 2
ENCODER_KERNEL_SIZE = 3
DECODER_KERNEL_SIZE = 3
ENCODER_DROPOUT = 0.5
DECODER_DROPOUT = 0.5

# Загрузка данных
spacy_en = spacy.load('en_core_web_sm')
spacy_ru = spacy.load('ru_core_news_sm')


def tokenize_en(text):
    return [tok.text for tok in spacy_en.tokenizer(text)]


def tokenize_ru(text):
    return [tok.text for tok in spacy_ru.tokenizer(text)]


SRC = Field(tokenize=tokenize_en,
            init_token='<sos>',
            eos_token='<eos>',
            lower=True)

TRG = Field(tokenize=tokenize_ru,
            init_token='<sos>',
            eos_token='<eos>',
            lower=True)

train_data, valid_data, test_data = Multi30k.splits(exts=('.en', '.ru'),
                                                    fields=(SRC, TRG))


class Encoder(nn.Module):
    def __init__(self, input_dim, embedding_dim, hidden_dim, num_layers, kernel_size, dropout):
        super().__init__()

        self.embedding = nn.Embedding(input_dim, embedding_dim)
        self.conv_layers = nn.ModuleList([nn.Conv1d(hidden_dim if i != 0 else embedding_dim,
                                                    hidden_dim,
                                                    kernel_size,
                                                    padding=(kernel_size - 1) // 2)
                                          for i in range(num_layers)])
        self.dropout = nn.Dropout(dropout)

    def forward(self, src):
        # src = [src_len, batch_size]
        embedded = self.dropout(self.embedding(src))
        # embedded = [src_len, batch_size, emb_dim]
        embedded = embedded.permute(1, 2, 0)
        # embedded = [batch_size, emb_dim, src_len]
        for i, conv_layer in enumerate(self.conv_layers):
            conved = conv_layer(self.dropout(embedded))
            conved = nn.functional.glu(conved, dim=1)
            conved = nn.functional.max_pool1d(conved, kernel_size=2)
            embedded = embedded + conved
        # embedded = [batch_size, hidden_dim, src_len]
        embedded = embedded.permute(2, 0, 1)
        return embedded


class Decoder(nn.Module):
    def __init__(self, output_dim, embedding_dim, hidden_dim, num_layers, kernel_size, dropout):
        super().__init__()

        self.output_dim = output_dim
        self.embedding = nn.Embedding(output_dim, embedding_dim)
        self.conv_layers = nn.ModuleList([nn.Conv1d(hidden_dim if i != 0 else embedding_dim,
                                                    hidden_dim,
                                                    kernel_size,
                                                    padding=(kernel_size - 1) // 2)
                                          for i in range(num_layers)])
        self.fc = nn.Linear(hidden_dim, output_dim)
        self.dropout = nn.Dropout(dropout)

        def forward(self, trg, encoder_output):
            # trg = [trg_len, batch_size]
            # encoder_output = [src_len, batch_size, hidden_dim]
        embedded = self.dropout(self.embedding(trg))
        # embedded = [trg_len, batch_size, emb_dim]
        embedded = embedded.permute(1, 2, 0)
        # embedded = [batch_size, emb_dim, trg_len]
        for i, conv_layer in enumerate(self.conv_layers):
            conved = conv_layer(self.dropout(embedded))
            conved = nn.functional.glu(conved, dim=1)
            conved = nn.functional.max_pool1d(conved, kernel_size=2)
            embedded = embedded + conved
            embedded = [batch_size, hidden_dim, trg_len]

    embedded = embedded.permute(2, 0, 1)
    # embedded = [trg_len, batch_size, hidden_dim]
    output = self.fc(self.dropout(embedded))
    # output = [trg_len, batch_size, output_dim]
    return output, None


# Определение модели и оптимизатора
INPUT_DIM = len(SRC.vocab)
OUTPUT_DIM = len(TRG.vocab)

encoder = Encoder(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, ENCODER_NUM_LAYERS, ENCODER_KERNEL_SIZE, ENCODER_DROPOUT)
decoder = Decoder(OUTPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, DECODER_NUM_LAYERS, DECODER_KERNEL_SIZE, DECODER_DROPOUT)

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

model = nn.Transformer(encoder, decoder)
model = model.to(device)

optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)
# Обучение
train_iterator, valid_iterator, test_iterator = BucketIterator.splits((train_data, valid_data, test_data), 
                                                                       batch_size = BATCH_SIZE, 
                                                                       device = device)

def train(model, iterator, optimizer, criterion, clip):
    model.train()
    epoch_loss = 0

    for i, batch in enumerate(iterator):
        src = batch.src
        trg = batch.trg

        optimizer.zero_grad()

        output = model(src, trg)

        output_dim = output.shape[-1]

        output = output[1:].view(-1, output_dim)
        trg = trg[1:].view(-1)

        loss = criterion(output, trg)

        loss.backward()

        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)

        optimizer.step()

        epoch_loss += loss.item()

    return epoch_loss / len(iterator)

def evaluate(model, iterator, criterion):
    model.eval()
    epoch_loss = 0

    with torch.no_grad():

        for i, batch in enumerate(iterator):
            src = batch.src
            trg = batch.trg

            output = model(src, trg)

            output_dim = output.shape[-1]

            output = output[1:].view(-1, output_dim)
            trg = trg[1:].view(-1)

            loss = criterion(output, trg)

            epoch_loss += loss.item()

    return epoch_loss / len(iterator)

criterion = nn.CrossEntropyLoss(ignore_index = TRG.vocab.stoi['<pad>'])

for epoch in range(NUM_EPOCHS):
    train_loss = train(model, train_iterator, optimizer, criterion, clip)
    valid_loss = evaluate(model, valid_iterator, criterion)

    print(f'Epoch: {epoch+1:02} | Train Loss: {train_loss:.3f} | Val. Loss: {valid_loss:.3f}')
